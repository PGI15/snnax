"use strict";(self.webpackChunksnnax_docs=self.webpackChunksnnax_docs||[]).push([[893],{8755:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var s=t(4848),i=t(8453);const r={slug:"/"},a="Introduction",o={id:"intro",title:"Introduction",description:"image",source:"@site/content/000_intro.md",sourceDirName:".",slug:"/",permalink:"/snnax/",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:0,frontMatter:{slug:"/"},sidebar:"tutorialSidebar",next:{title:"Installation",permalink:"/snnax/gettingStarted/installation"}},l={},d=[{value:"Quick Start",id:"quick-start",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",img:"img",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"image",src:t(9904).A+"",width:"754",height:"265"})}),"\n",(0,s.jsxs)(n.p,{children:["SNNAX is a lightweight library built on ",(0,s.jsx)(n.strong,{children:"Equinox"})," and ",(0,s.jsx)(n.strong,{children:"JAX"})," to provide a spiking neural network (SNN) simulator for deep learning. It is designed to be user-friendly and flexible, allowing users to define custom SNN layers while leveraging common deep learning layers from Equinox. SNNAX is fully compatible with JAX, enabling the use of JAX's function transformation features like vectorization with ",(0,s.jsx)(n.code,{children:"jax.vmap"}),", automatic differentiation, and JIT compilation with XLA."]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,s.jsxs)(n.p,{children:["The following code demonstrates how to define a simple SNN in SNNAX using the ",(0,s.jsx)(n.a,{href:"/snnax/architecture/composed#sequential",children:(0,s.jsx)(n.code,{children:"snnax.snn.Sequential"})})," class to stack layers of SNNs and Equinox layers into a feed-forward architecture:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import jax\r\nimport jax.numpy as jnp\r\n\r\nimport equinox as eqx\r\nimport snnax.snn as snn\r\n\r\nimport optax\r\n\r\nmodel = snn.Sequential(eqx.Conv2D(2, 32, 7, 2, key=key1),\r\n                        snn.LIF((8, 8), [.9, .8], key=key2),\r\n                        snn.flatten(),\r\n                        eqx.Linear(64, 11, key=key3),\r\n                        snn.LIF(11, [.9, .8], key=key4))\n"})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR;\r\n    X--\x3eConv2D;\r\n    Conv2D--\x3eLIF1[LIF];\r\n    LIF1[LIF]--\x3eFlatten;\r\n    Flatten--\x3eLinear;\r\n    Linear--\x3eLIF2[LIF];\r\n    LIF2[LIF]--\x3eY;"}),"\n",(0,s.jsxs)(n.p,{children:["Next, define a loss function for a single sample and use JAX's vectorization features to create a batched loss function.\r\nNote that the model's output is a tuple of membrane potentials and spikes. For a feed-forward SNN, use the last element of the spike list, ",(0,s.jsx)(n.code,{children:"out_spikes[-1]"}),", and sum the spikes across time to get the spike count."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Simple batched loss function\r\n\r\n@partial(jax.vmap, in_axes=(0, 0, 0))\r\ndef loss_fn(in_states, in_spikes, tgt_class):\r\nout_state, out_spikes = model(in_states, in_spikes)\r\n\r\n    # Spikes from the last layer are summed across time\r\n    pred = out_spikes[-1].sum(-1)\r\n    loss = optax.softmax_cross_entropy(pred, tgt_class)\r\n    return loss\r\n\r\n\r\n# Calculating the gradient with Equinox PyTree filters and\r\n# subsequently jitting the resulting function\r\n@eqx.filter_jit\r\n@eqx.filter_value_and_grad\r\ndef loss_and_grad(in_states, in_spikes, tgt_class):\r\nreturn jnp.mean(loss_fn(in_states, in_spikes, tgt_class))\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Finally, train the model by feeding it input spike trains and states. Initialize the states of the SNN using the ",(0,s.jsx)(n.code,{children:"init_states"})," method of the ",(0,s.jsx)(n.a,{href:"/snnax/architecture/composed#sequential",children:(0,s.jsx)(n.code,{children:"snnax.snn.Sequential"})})," class."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# ...\r\n# Simple training loop\r\n\r\nfor in_spikes, tgt_class in tqdm(dataloader):\r\n    # Initializing the membrane potentials of LIF neurons\r\n    states = model.init_states(key)\r\n\r\n    # Jitting with Equinox PyTree filters\r\n    loss, grads = loss_and_grad(states, in_spikes, tgt_class)\r\n\r\n    # Update parameter PyTree with Equinox and optax\r\n    updates, opt_state = optim.update(grads, opt_state)\r\n    model = eqx.apply_updates(model, updates)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Fully worked-out examples can be found in the examples directory."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},9904:(e,n,t)=>{t.d(n,{A:()=>s});const s=t.p+"assets/images/snnax-4cfaaab0f3b9e0ec73e72d099d1c252f.svg"}}]);